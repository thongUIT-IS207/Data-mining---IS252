{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Tên   Bằng cấp Kinh nghiệm Tiếng Anh    Phỏng vấn Tuyển dụng\n",
      "0   Trâm    Đại học       Nhiều      Biết  Bình thường    Từ chối\n",
      "1  Trinh    Đại học       Nhiều      Biết     Xuất sắc  Chấp nhận\n",
      "2  Trưng    Cao học       Nhiều     Không  Bình thường  Chấp nhận\n",
      "3   Tuấn  Trung cấp          Ít      Biết          Tốt    Từ chối\n",
      "4     Tú    Cao học  Trung bình      Biết     Xuất sắc  Chấp nhận\n",
      "5    Tâm    Đại học  Trung bình      Biết  Bình thường    Từ chối\n",
      "6   Tùng    Cao học          Ít      Biết  Bình thường    Từ chối\n",
      "7   Toàn  Trung cấp          Ít     Không     Xuất sắc    Từ chối\n",
      "Xấp xỉ dưới: {0}\n",
      "Xấp xỉ trên: {0, 1, 2, 5}\n",
      "Phụ thuộc của {Tuyển dụng} vào {Bằng cấp, Phỏng vấn}: không phụ thuộc\n",
      "Rút gọn của bảng quyết định: ['Tên', 'Bằng cấp', 'Kinh nghiệm', 'Tiếng Anh', 'Phỏng vấn']\n",
      "3 luật phân loại: ['Nếu Bằng cấp = Đại học và Kinh nghiệm = Nhiều thì Tuyển dụng = Từ chối', 'Nếu Bằng cấp = Cao học và Phỏng vấn = Xuất sắc thì Tuyển dụng = Chấp nhận', 'Nếu Bằng cấp = Trung cấp và Kinh nghiệm = Ít thì Tuyển dụng = Từ chối']\n"
     ]
    }
   ],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu từ file Excel\n",
    "file_path = 'D:/school lecture/New folder/English.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Hiển thị dữ liệu để kiểm tra\n",
    "print(df)\n",
    "\n",
    "# Bài 1: Các đối tượng X = {Trâm, Tuấn, Tùng, Toàn}\n",
    "X_set = df[(df['Tên'].isin(['Trâm', 'Tuấn', 'Tùng', 'Toàn']))].index.tolist()\n",
    "\n",
    "# Các thuộc tính B = {Bằng cấp, Kinh nghiệm}\n",
    "B_attributes = ['Bằng cấp', 'Kinh nghiệm']\n",
    "\n",
    "# Bước 1: Tính xấp xỉ dưới và xấp xỉ trên\n",
    "def lower_approximation(df, B_attributes, X_set):\n",
    "    lower_approx = set()\n",
    "    for i, row in df.iterrows():\n",
    "        if all(row[attr] == df.loc[X_set[0], attr] for attr in B_attributes):\n",
    "            if row.name in X_set:\n",
    "                lower_approx.add(row.name)\n",
    "    return lower_approx\n",
    "\n",
    "def upper_approximation(df, B_attributes, X_set):\n",
    "    upper_approx = set()\n",
    "    for i, row in df.iterrows():\n",
    "        if any(row[attr] == df.loc[X_set[0], attr] for attr in B_attributes):\n",
    "            upper_approx.add(row.name)\n",
    "    return upper_approx\n",
    "\n",
    "# Tính xấp xỉ dưới và xấp xỉ trên của tập X\n",
    "lower = lower_approximation(df, B_attributes, X_set)\n",
    "upper = upper_approximation(df, B_attributes, X_set)\n",
    "\n",
    "print(\"Xấp xỉ dưới:\", lower)\n",
    "print(\"Xấp xỉ trên:\", upper)\n",
    "\n",
    "# Bước 2: Tính độ phụ thuộc của C = {Tuyển dụng} vào B = {Bằng cấp, Phỏng vấn}\n",
    "def attribute_dependence(df, C_attributes, B_attributes):\n",
    "    ind_B = df.groupby(B_attributes).groups\n",
    "    ind_C = df.groupby(C_attributes).groups\n",
    "    dependent = True\n",
    "    for group in ind_C.values():\n",
    "        if not any(set(group).issubset(set(val)) for val in ind_B.values()):\n",
    "            dependent = False\n",
    "            break\n",
    "    return dependent\n",
    "\n",
    "\n",
    "# Phụ thuộc thuộc tính\n",
    "dependent = attribute_dependence(df, ['Tuyển dụng'], ['Bằng cấp', 'Phỏng vấn'])\n",
    "if(dependent == True ):\n",
    "    print(\"Phụ thuộc của {Tuyển dụng} vào {Bằng cấp, Phỏng vấn}: phụ thuộc\")\n",
    "else:\n",
    "    print(\"Phụ thuộc của {Tuyển dụng} vào {Bằng cấp, Phỏng vấn}: không phụ thuộc\")\n",
    "\n",
    "# Bước 3: Tìm các rút gọn\n",
    "def find_reduct(df, decision_attribute):\n",
    "    # Tính ma trận phân biệt\n",
    "    columns = df.columns.tolist()\n",
    "    reducts = []\n",
    "    for col in columns:\n",
    "        if col != decision_attribute:\n",
    "            reducts.append(col)\n",
    "    return reducts\n",
    "\n",
    "# Tìm rút gọn cho thuộc tính 'Recruitment'\n",
    "reducts = find_reduct(df, 'Tuyển dụng')\n",
    "print(\"Rút gọn của bảng quyết định:\", reducts)\n",
    "\n",
    "# Đề xuất 3 luật phân loại chính xác 100% từ rút gọn\n",
    "rules = [\n",
    "    \"Nếu Bằng cấp = Đại học và Kinh nghiệm = Nhiều thì Tuyển dụng = Từ chối\",\n",
    "    \"Nếu Bằng cấp = Cao học và Phỏng vấn = Xuất sắc thì Tuyển dụng = Chấp nhận\",\n",
    "    \"Nếu Bằng cấp = Trung cấp và Kinh nghiệm = Ít thì Tuyển dụng = Từ chối\"\n",
    "]\n",
    "print(\"3 luật phân loại:\", rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 -> Troi: Có sự phụ thuộc\n",
      "Unnamed: 0 -> Gio: Có sự phụ thuộc\n",
      "Unnamed: 0 -> Apsuat: Có sự phụ thuộc\n",
      "Unnamed: 0 -> Ketqua: Có sự phụ thuộc\n",
      "Troi -> Unnamed: 0: Không có sự phụ thuộc\n",
      "Troi -> Gio: Không có sự phụ thuộc\n",
      "Troi -> Apsuat: Không có sự phụ thuộc\n",
      "Troi -> Ketqua: Không có sự phụ thuộc\n",
      "Gio -> Unnamed: 0: Không có sự phụ thuộc\n",
      "Gio -> Troi: Không có sự phụ thuộc\n",
      "Gio -> Apsuat: Không có sự phụ thuộc\n",
      "Gio -> Ketqua: Không có sự phụ thuộc\n",
      "Apsuat -> Unnamed: 0: Không có sự phụ thuộc\n",
      "Apsuat -> Troi: Không có sự phụ thuộc\n",
      "Apsuat -> Gio: Không có sự phụ thuộc\n",
      "Apsuat -> Ketqua: Không có sự phụ thuộc\n",
      "Ketqua -> Unnamed: 0: Không có sự phụ thuộc\n",
      "Ketqua -> Troi: Không có sự phụ thuộc\n",
      "Ketqua -> Gio: Không có sự phụ thuộc\n",
      "Ketqua -> Apsuat: Không có sự phụ thuộc\n"
     ]
    }
   ],
   "source": [
    "# Đọc dữ liệu từ file Excel\n",
    "df = pd.read_excel('D:/school lecture/New folder/data.xlsx', engine='openpyxl')\n",
    "\n",
    "# Tính toán sự phụ thuộc tính\n",
    "def calculate_dependency(df):\n",
    "    dependencies = {}\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            if col1 != col2:\n",
    "                # Kiểm tra sự phụ thuộc\n",
    "                dependency = df.groupby(col1)[col2].nunique() == 1\n",
    "                dependencies[(col1, col2)] = dependency.all()\n",
    "    return dependencies\n",
    "\n",
    "dependencies = calculate_dependency(df)\n",
    "\n",
    "# In kết quả\n",
    "for (col1, col2), dep in dependencies.items():\n",
    "    if dep:\n",
    "        print(f\"{col1} -> {col2}: Có sự phụ thuộc\")\n",
    "    else:\n",
    "        print(f\"{col1} -> {col2}: Không có sự phụ thuộc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('Unnamed: 0',)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [reduct \u001b[38;5;28;01mfor\u001b[39;00m reduct \u001b[38;5;129;01min\u001b[39;00m minimal_reducts \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reduct) \u001b[38;5;241m==\u001b[39m min_length]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Tìm tập rút gọn thuộc tính\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m reducts \u001b[38;5;241m=\u001b[39m find_reduct(df)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# In kết quả\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTập rút gọn tối thiểu:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[45], line 20\u001b[0m, in \u001b[0;36mfind_reduct\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m combo \u001b[38;5;129;01min\u001b[39;00m combinations(columns, r):\n\u001b[1;32m---> 20\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m distinguishable(df, combo):\n\u001b[0;32m     21\u001b[0m             minimal_reducts\u001b[38;5;241m.\u001b[39mappend(combo)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Lọc ra tập rút gọn tối thiểu (có kích thước nhỏ nhất)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[45], line 9\u001b[0m, in \u001b[0;36mdistinguishable\u001b[1;34m(df, attributes)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistinguishable\u001b[39m(df, attributes):\n\u001b[1;32m----> 9\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(attributes)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKết quả\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(grouped \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\thong\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   8870\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   8871\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   8872\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   8873\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   8874\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   8875\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   8876\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   8877\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   8878\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   8879\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thong\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1279\u001b[0m         obj,\n\u001b[0;32m   1280\u001b[0m         keys,\n\u001b[0;32m   1281\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1282\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1283\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1284\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1285\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1286\u001b[0m     )\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32mc:\\Users\\thong\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: ('Unnamed: 0',)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Đọc dữ liệu từ file Excel\n",
    "df = pd.read_excel('D:/school lecture/New folder/data.xlsx', engine='openpyxl')\n",
    "\n",
    "# Hàm kiểm tra khả năng phân biệt theo tập thuộc tính cho trước\n",
    "def distinguishable(df, attributes):\n",
    "    grouped = df.groupby(attributes)['Kết quả'].nunique()\n",
    "    return all(grouped == 1)\n",
    "\n",
    "# Hàm tìm tập rút gọn thuộc tính\n",
    "def find_reduct(df):\n",
    "    columns = list(df.columns[:-1])  # Loại bỏ cột 'Kết quả'\n",
    "    minimal_reducts = []\n",
    "\n",
    "    # Tìm tất cả các tổ hợp thuộc tính\n",
    "    for r in range(1, len(columns) + 1):\n",
    "        for combo in combinations(columns, r):\n",
    "            if distinguishable(df, combo):\n",
    "                minimal_reducts.append(combo)\n",
    "\n",
    "    # Lọc ra tập rút gọn tối thiểu (có kích thước nhỏ nhất)\n",
    "    min_length = min(len(reduct) for reduct in minimal_reducts)\n",
    "    return [reduct for reduct in minimal_reducts if len(reduct) == min_length]\n",
    "\n",
    "# Tìm tập rút gọn thuộc tính\n",
    "reducts = find_reduct(df)\n",
    "\n",
    "# In kết quả\n",
    "print(\"Tập rút gọn tối thiểu:\")\n",
    "for reduct in reducts:\n",
    "    print(reduct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('', engine='openpyxl')\n",
    "\n",
    "def find_outer_boundary(df):\n",
    "    unique_values = set(df['Ketqua'])\n",
    "        outer_boundary = []\n",
    "        for value in unique_values:\n",
    "            subset = df[df['Ketqua'] == value]\n",
    "        if subset.shape[0] == 1:  # Nếu chỉ có một đối tượng cho giá trị này\n",
    "            outer_boundary.append(value)\n",
    "    return outer_boundary\n",
    "\n",
    "outer_boundary = find_outer_boundary(df)\n",
    "\n",
    "print(\"Vùng biên ngoài:\", outer_boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('C:/New File Downloads/data.xlsx', engine='openpyxl')\n",
    "X = {'O1', 'O3', 'O4'}\n",
    "B = ['Troi', 'Gio']\n",
    "\n",
    "def lower_approximation(df, X, B):\n",
    "    \"\"\"Tính xấp xỉ dưới của tập X.\"\"\"\n",
    "    lower = set()\n",
    "    for _, row in df.iterrows():\n",
    "        matching_objects = df[(df[B] == row[B].values).all(axis=1)]['Ketqua']\n",
    "        if set(matching_objects).issubset(X):\n",
    "            lower.add(row['Ketqua'])\n",
    "    return lower\n",
    "\n",
    "def upper_approximation(df, X, B):\n",
    "    \"\"\"Tính xấp xỉ trên của tập X.\"\"\"\n",
    "    upper = set()\n",
    "    for _, row in df.iterrows():\n",
    "        matching_objects = df[(df[B] == row[B].values).all(axis=1)]['Ketqua']\n",
    "        if set(matching_objects).intersection(X):\n",
    "            upper.add(row['Ketqua'])\n",
    "    return upper\n",
    "\n",
    "lower = lower_approximation(df, X, B)\n",
    "upper = upper_approximation(df, X, B)\n",
    "\n",
    "boundary_region = upper - lower\n",
    "\n",
    "print(\"Xấp xỉ dưới:\", lower)\n",
    "print(\"Xấp xỉ trên:\", upper)\n",
    "print(\"Vùng biên:\", boundary_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    #  a  b  c  d  e\n",
      "0  O1  1  0  2  2  0\n",
      "1  O2  6  8  3  5  2\n",
      "2  O3  2  0  0  5  1\n",
      "3  O4  1  8  0  2  2\n",
      "4  O5  1  0  2  4  1\n",
      "Lower Approximation: []\n",
      "Upper Approximation: ['O1', 'O2', 'O3', 'O5', 'O6', 'O8']\n",
      "Dependency of C={e} on B={a, b}: 0.00%\n",
      "Checking subset ('a',): Dependency = 25.00%\n",
      "Checking subset ('b',): Dependency = 62.50%\n",
      "Checking subset ('a', 'b'): Dependency = 0.00%\n",
      "Reducts: []\n",
      "No reducts found, hence no rules can be generated.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Load the decision table from Excel\n",
    "file_path = 'D:/school lecture/New folder/task2-data.xlsx'  # Update the path to your Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows to ensure data is loaded correctly\n",
    "print(df.head())\n",
    "\n",
    "# Assuming the structure of the decision table in Excel is the same:\n",
    "X = df[df['e'] == 1]['#'].values  # This gives objects with e == 1\n",
    "attributes = ['a', 'b']  # B = {a, b}\n",
    "\n",
    "# Step 1: Calculate the lower and upper approximations\n",
    "def lower_approximation(df, X, attributes):\n",
    "    lower_approx = []\n",
    "    for obj1 in X:\n",
    "        obj1_row = df[df['#'] == obj1]\n",
    "        matches_all = True\n",
    "        for obj2 in X:\n",
    "            if obj1 != obj2:\n",
    "                obj2_row = df[df['#'] == obj2]\n",
    "                if not all(obj1_row[attr].values == obj2_row[attr].values for attr in attributes):\n",
    "                    matches_all = False\n",
    "                    break\n",
    "        if matches_all:\n",
    "            lower_approx.append(obj1)\n",
    "    return lower_approx\n",
    "\n",
    "def upper_approximation(df, X, attributes):\n",
    "    upper_approx = []\n",
    "    for obj in df['#']:\n",
    "        obj_row = df[df['#'] == obj]\n",
    "        if any(all(obj_row[attr].values == df[df['#'] == x][attr].values for attr in attributes) for x in X):\n",
    "            upper_approx.append(obj)\n",
    "    return upper_approx\n",
    "\n",
    "lower_approx = lower_approximation(df, X, attributes)\n",
    "upper_approx = upper_approximation(df, X, attributes)\n",
    "\n",
    "print(\"Lower Approximation:\", lower_approx)\n",
    "print(\"Upper Approximation:\", upper_approx)\n",
    "\n",
    "# Step 2: Investigate dependency C = {e} on B = {a, b}\n",
    "def calculate_dependency(df, attributes, decision_attr='e'):\n",
    "    positive_region = set()\n",
    "    for obj1 in df['#']:\n",
    "        obj1_row = df[df['#'] == obj1]\n",
    "        matches = False\n",
    "        for obj2 in df['#']:\n",
    "            if obj1 != obj2:\n",
    "                obj2_row = df[df['#'] == obj2]\n",
    "                # Check if the objects are the same based on the attributes\n",
    "                if all(obj1_row[attr].values == obj2_row[attr].values for attr in attributes):\n",
    "                    # If the objects have the same decision attribute value, count it as a match\n",
    "                    if obj1_row[decision_attr].values == obj2_row[decision_attr].values:\n",
    "                        matches = True\n",
    "                        positive_region.add(obj1)\n",
    "                        break\n",
    "    dependency = len(positive_region) / len(df)\n",
    "    return dependency\n",
    "\n",
    "dependency = calculate_dependency(df, attributes)\n",
    "print(f\"Dependency of C={{e}} on B={{a, b}}: {dependency * 100:.2f}%\")\n",
    "\n",
    "# Step 3: Find reducts\n",
    "def find_reducts(df, attributes, decision_attr='e'):\n",
    "    reducts = []\n",
    "    for r in range(1, len(attributes) + 1):\n",
    "        for subset in combinations(attributes, r):\n",
    "            dependency = calculate_dependency(df, subset, decision_attr)\n",
    "            print(f\"Checking subset {subset}: Dependency = {dependency * 100:.2f}%\")  # Debug output\n",
    "            if dependency == 1.0:\n",
    "                reducts.append(subset)\n",
    "    return reducts\n",
    "\n",
    "reducts = find_reducts(df, attributes)\n",
    "print(\"Reducts:\", reducts)\n",
    "\n",
    "# Step 4: Propose 3 decision rules with 100% accuracy\n",
    "def generate_rules(df, reducts):\n",
    "    rules = []\n",
    "    for reduct in reducts:\n",
    "        for _, row in df.iterrows():\n",
    "            condition = \" & \".join(f\"{attr}={row[attr]}\" for attr in reduct)\n",
    "            rule = f\"If {condition}, then e={row['e']}\"\n",
    "            rules.append(rule)\n",
    "    return rules[:3]  # Return 3 rules\n",
    "\n",
    "if reducts:\n",
    "    rules = generate_rules(df, reducts)\n",
    "    print(\"\\nGenerated Decision Rules:\")\n",
    "    for rule in rules:\n",
    "        print(rule)\n",
    "else:\n",
    "    print(\"No reducts found, hence no rules can be generated.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
