import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from streamlit_carousel import carousel
from sklearn.datasets import load_iris, make_blobs
from sklearn.datasets import load_breast_cancer, load_wine
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from mlxtend.frequent_patterns import apriori, association_rules
import time
from minisom import MiniSom
from itertools import combinations

st.markdown(
    """
    <style>
        body {
            background-color: #f4f4f4;
            color: #333333;
        }
        .stButton > button {
            width: 100%;
            background-color: #4CAF50;
            color: white;
            font-size: 1.2em;
            border-radius: 12px;
            padding: 10px;
            transition: background-color 0.3s, transform 0.2s;
        }
        .stButton > button:hover {
            background-color: #45a049;
            transform: translateY(-3px);
        }
        .stHeader {
            background-color: #4caf50;
            color: white;
            padding: 10px;
            border-radius: 8px;
        }
        .stCard {
            background-color: #f4f4f4;
            border-radius: 15px;
            padding: 20px;
            box-shadow: 0px 4px 15px rgba(0, 0, 0, 0.1);
            margin: 10px 0;
            text-align: center;
            transition: all 0.3s ease;
        }
        .stCard:hover {
            background-color: #e6f7ff;
            box-shadow: 0px 6px 20px rgba(0, 0, 0, 0.15);
            transform: translateY(-5px);
        }
        .title {
            font-size: 2.5em;
            text-align: center;
            color: #4CAF50;
            margin-bottom: 1em;
        }
        .stCard h3 {
            color: #007ACC;
            font-size: 1.5em;
            margin-bottom: 10px;
        }
        .stCard p {
            color: #555;
            font-size: 1em;
        }
        .button-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }
        .carousel-container {
            margin-top: 20px;
            margin-bottom: 40px;
        }
    </style>
    """,
    unsafe_allow_html=True
)
# Header
st.markdown("<h1 class='stHeader'>üåà ·ª®ng D·ª•ng Data Mining</h1>", unsafe_allow_html=True)
# H√†m hi·ªÉn th·ªã giao di·ªán ch√≠nh
def main_page():
    st.markdown("<h2>üè† Trang Ch·ªß</h2>", unsafe_allow_html=True)
    st.write("Ch√†o m·ª´ng b·∫°n ƒë·∫øn v·ªõi ·ª©ng d·ª•ng Data Mining!")

    st.markdown("<div class='carousel-container'>", unsafe_allow_html=True)

    carousel_items = [
    {"title": "üõ†Ô∏è Data Preprocessing", "text": "Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·ªÉ l√†m s·∫°ch v√† chu·∫©n h√≥a tr∆∞·ªõc khi ph√¢n t√≠ch.", "img": "https://via.placeholder.com/600x300?text=Data+Preprocessing"},
    {"title": "üå≥ Decision Tree", "text": "X√¢y d·ª±ng v√† tr·ª±c quan h√≥a c√¢y quy·∫øt ƒë·ªãnh ƒë·ªÉ ph√¢n lo·∫°i d·ªØ li·ªáu.", "img": "https://via.placeholder.com/600x300?text=Decision+Tree"},
    {"title": "üìä Rough Set Analysis", "text": "Ph√¢n t√≠ch t·∫≠p th√¥ ƒë·ªÉ t√¨m lu·∫≠t v√† r√∫t g·ªçn d·ªØ li·ªáu.", "img": "https://via.placeholder.com/600x300?text=Rough+Set"},
    {"title": "üìâ K-Means Clustering", "text": "Ph√¢n c·ª•m d·ªØ li·ªáu th√†nh c√°c nh√≥m b·∫±ng thu·∫≠t to√°n K-Means.", "img": "https://via.placeholder.com/600x300?text=K-Means+Clustering"},
    {"title": "ü§ñ Naive Bayes", "text": "Ph√¢n lo·∫°i d·ªØ li·ªáu b·∫±ng thu·∫≠t to√°n Naive Bayes.", "img": "https://via.placeholder.com/600x300?text=Naive+Bayes"}
    ]

    carousel(items=carousel_items, width=600)
    st.markdown("</div>", unsafe_allow_html=True)
    # Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u trong card
    st.markdown("<div class='stCard'><h3>üìä D·ªØ li·ªáu M·∫´u</h3></div>", unsafe_allow_html=True)
    st.write(df)
    st.write("Ch·ªçn m·ªôt thu·∫≠t to√°n ƒë·ªÉ kh√°m ph√°:")
    
    algorithms = {
        "üõ†Ô∏è Data Preprocessing": preprocessing_page,
        "üå≥ Decision Tree": decision_tree_page,
        "üìä Rough Set Analysis": rough_set_page,
        "üìâ K-Means Clustering": kmeans_page,
        "üîó Apriori Association Rule": apriori_page,
        "üß† Kohonen Self-Organizing Map": kohonen_page,
        "ü§ñ Naive Bayes Classification": naive_bayes_page,
        "üå≤ Random Forest": random_forest_page
    }
    num_cols = 2
    st.markdown("<div class='button-container'>", unsafe_allow_html=True)
    algo_names = list(algorithms.keys())
    for i in range(0, len(algo_names), num_cols):
        cols = st.columns(num_cols)
        for col, algo_name in zip(cols, algo_names[i:i + num_cols]):
            if col.button(algo_name):
                st.session_state["current_page"] = algorithms[algo_name]
                st.rerun()
    st.markdown("</div>", unsafe_allow_html=True)
# D·ªØ li·ªáu fix c·ª©ng
@st.cache_data
def load_data():
    data = {
        'Age': [25, 32, 47, 51, 62, 28, 35, 43, 52, 45],
        'Income': [50000, 60000, 80000, 120000, 150000, 52000, 58000, 79000, 115000, 87000],
        'Education': ['Bachelors', 'Masters', 'PhD', 'PhD', 'Masters', 'Bachelors', 'Bachelors', 'Masters', 'PhD', 'Masters'],
        'Purchase': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes']
    }
    return pd.DataFrame(data)

df = load_data()
# 0.data preprocessing
def preprocessing_page():
    st.title("üõ†Ô∏è Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")
    st.header("1Ô∏è‚É£ T·∫£i T·∫≠p D·ªØ Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)

        # Hi·ªÉn th·ªã d·ªØ li·ªáu
        st.write("### üîç D·ªØ li·ªáu ban ƒë·∫ßu:")
        st.write(df.head())
        st.write(f"**S·ªë l∆∞·ª£ng m·∫´u:** {df.shape[0]}, **S·ªë l∆∞·ª£ng ƒë·∫∑c tr∆∞ng:** {df.shape[1]}")

        # ==========================
        # 2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
        # ==========================
        st.sidebar.header("üõ†Ô∏è Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu")

        remove_missing = st.sidebar.checkbox("Lo·∫°i b·ªè gi√° tr·ªã thi·∫øu")
        standardize = st.sidebar.checkbox("Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler)")
        remove_outliers = st.sidebar.checkbox("X·ª≠ l√Ω gi√° tr·ªã ngo·∫°i lai (IQR)")
        filter_noise = st.sidebar.checkbox("X·ª≠ l√Ω nhi·ªÖu (L·ªçc trung b√¨nh)")
        encode_labels = st.sidebar.checkbox("M√£ h√≥a nh√£n (Label Encoding)")

        # ==========================
        # 3. N√∫t Ti·ªÅn X·ª≠ L√Ω
        # ==========================
        if st.sidebar.button("üöÄ Ti·∫øn h√†nh Ti·ªÅn X·ª≠ L√Ω"):
            # Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
            if remove_missing:
                # 1. X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
                df["Age"].fillna(df["Age"].median(), inplace=True)  # ƒêi·ªÅn median cho Age
                df["Discount"].fillna(0, inplace=True)  # ƒêi·ªÅn 0 cho Discount
                df["OrderDate"] = pd.to_datetime(df["OrderDate"]).fillna(pd.Timestamp("2021-01-01"))  # ƒêi·ªÅn gi√° tr·ªã m·∫∑c ƒë·ªãnh cho OrderDate
                df["City"].fillna("Unknown", inplace=True)  # ƒêi·ªÅn "Unknown" cho City
                df["Rating"].fillna(df["Rating"].mean(), inplace=True)  # ƒêi·ªÅn gi√° tr·ªã trung b√¨nh cho Rating
                st.success("‚úÖ ƒê√£ lo·∫°i b·ªè gi√° tr·ªã thi·∫øu.")

            # 3. Chu·∫©n h√≥a d·ªØ li·ªáu
            if standardize:
                scaler = StandardScaler()
                numeric_columns = ['Age', 'Price', 'Discount', 'Rating']
                df[numeric_columns] = scaler.fit_transform(df[numeric_columns])
                st.success("‚úÖ ƒê√£ chu·∫©n h√≥a d·ªØ li·ªáu.")

            if remove_outliers:
                numeric_columns = ['Age', 'Price', 'Discount', 'Rating']
                for col in numeric_columns:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    df = df[~((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR)))]
                st.success("‚úÖ ƒê√£ x·ª≠ l√Ω gi√° tr·ªã ngo·∫°i lai (IQR).")


            # 4. X·ª≠ l√Ω nhi·ªÖu
            if filter_noise:
                numeric_columns = ['Age', 'Price', 'Discount', 'Rating']
                df[numeric_columns] = df[numeric_columns].rolling(window=3, min_periods=1).mean()
                st.success("‚úÖ ƒê√£ x·ª≠ l√Ω nhi·ªÖu (l·ªçc trung b√¨nh).")

            # 5. M√£ h√≥a nh√£n
            if encode_labels:
                categorical_columns = ['CustomerName', 'Product', 'Category', 'City', 'State', 'Country']
                encoder = LabelEncoder()
                for col in categorical_columns:
                    df[col] = encoder.fit_transform(df[col])
                st.success("‚úÖ ƒê√£ m√£ h√≥a nh√£n.")

            st.write("### üõ†Ô∏è D·ªØ li·ªáu sau ti·ªÅn x·ª≠ l√Ω:")
            st.write(df.head())
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()

# 1. Decision Tree
def decision_tree_page():
    st.markdown("<h2>üß† Decision Tree</h2>", unsafe_allow_html=True)
    st.header("1Ô∏è‚É£ T·∫£i T·∫≠p D·ªØ Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write(df)

        target = st.selectbox("Ch·ªçn c·ªôt m·ª•c ti√™u (label):", df.columns, index=3)
        features = st.multiselect("Ch·ªçn c·ªôt ƒë·∫∑c tr∆∞ng (features):", df.columns, default=['Outlook', 'Temperature'])

        if len(features) == 0:
            st.warning("‚ùó Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt c·ªôt ƒë·∫∑c tr∆∞ng.")
            return
        
        # Ch·ªçn ti√™u ch√≠ ph√¢n t√°ch (Gini ho·∫∑c Entropy)
        criterion = st.selectbox("Ch·ªçn ti√™u ch√≠ ph√¢n t√°ch:", ["gini", "entropy"], index=0)

        if st.button("üöÄ T·∫°o C√¢y Quy·∫øt ƒê·ªãnh"):
            with st.spinner("üîÑ ƒêang t·∫°o c√¢y quy·∫øt ƒë·ªãnh..."):
                time.sleep(2)  # Gi·∫£ l·∫≠p th·ªùi gian x·ª≠ l√Ω
                for column in features:
                    if df[column].dtype in ['object', 'string']:
                        df, notes = encode_column_with_notes(df, column)
                # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi ti√™u ch√≠ ƒë√£ ch·ªçn
                X = df[features]
                y = df[target]
                # Chu·∫©n h√≥a d·ªØ li·ªáu
                model = DecisionTreeClassifier(criterion=criterion, random_state=42)
                model.fit(X, y)

                st.success(f"‚úÖ C√¢y quy·∫øt ƒë·ªãnh ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng v·ªõi ti√™u ch√≠ '{criterion}'!")
                plt.figure(figsize=(12, 8))
                plot_tree(model, feature_names=features, class_names=model.classes_, filled=True)
                st.pyplot(plt)

                fig, ax = plt.subplots(figsize=(12, 8))  # K√≠ch th∆∞·ªõc bi·ªÉu ƒë·ªì
                plot_tree(
                    model, 
                    feature_names=features, 
                    class_names=[str(c) for c in model.classes_], 
                    filled=True, 
                    rounded=True,  # L√†m tr√≤n √¥ n√∫t
                    impurity=False,  # ·∫®n ch·ªâ s·ªë impurity
                    ax=ax
                )
                st.pyplot(fig)
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()

# thuat toan doi kieu chu thanh so
def encode_column_with_notes(df, column):
    """
    H√†m chuy·ªÉn ƒë·ªïi c·ªôt d·∫°ng chu·ªói th√†nh s·ªë v√† ghi l·∫°i ch√∫ th√≠ch.
    
    Args:
        df (pd.DataFrame): DataFrame c·∫ßn chuy·ªÉn ƒë·ªïi.
        column (str): T√™n c·ªôt c·∫ßn chuy·ªÉn ƒë·ªïi.

    Returns:
        df (pd.DataFrame): DataFrame sau khi chuy·ªÉn ƒë·ªïi.
        notes (dict): Ch√∫ th√≠ch ghi l·∫°i c√°c gi√° tr·ªã c≈© v√† gi√° tr·ªã s·ªë t∆∞∆°ng ·ª©ng.
    """
    le = LabelEncoder()
    original_values = df[column].unique()
    df[column] = le.fit_transform(df[column])
    encoded_values = df[column].unique()
    notes = {original: encoded for original, encoded in zip(original_values, encoded_values)}
    return df, notes

# 2. K-Means Clustering
def kmeans_page():
    st.markdown("<h2>üìä K-Means Clustering</h2>", unsafe_allow_html=True)
    st.header("1Ô∏è‚É£ T·∫£i T·∫≠p D·ªØ Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write(df)
        # Ng∆∞·ªùi d√πng ch·ªçn c√°c c·ªôt ƒë·ªÉ ph√¢n c·ª•m
        selected_features = st.multiselect("Ch·ªçn c√°c c·ªôt:", df.columns, default=["AnnualIncome", "SpendingScore", "LoyaltyScore"])
        num_clusters = st.slider("Ch·ªçn s·ªë c·ª•m (k):", 2, 5, 3)

        if st.button("üöÄ Th·ª±c Hi·ªán Ph√¢n C·ª•m"):
            if len(selected_features) < 2:
                st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ ph√¢n c·ª•m.")
            else:
                df1 = df.copy()# Sao ch√©p dataframe g·ªëc ƒë·ªÉ gi·ªØ nguy√™n d·ªØ li·ªáu ban ƒë·∫ßu
                column_notes = {}  # Bi·∫øn l∆∞u ch√∫ th√≠ch cho c√°c c·ªôt ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi

                X = df[selected_features]

                # Chu·∫©n h√≥a d·ªØ li·ªáu
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # Ki·ªÉm tra v√† chuy·ªÉn ƒë·ªïi c√°c thu·ªôc t√≠nh d·∫°ng chu·ªói th√†nh d·∫°ng s·ªë
                for column in selected_features:
                    if df1[column].dtype in ['object', 'string']:
                        df1, notes = encode_column_with_notes(df1, column)
                        column_notes[column] = notes  # L∆∞u ch√∫ th√≠ch v√†o dictionary

                with st.spinner("üîÑ ƒêang th·ª±c hi·ªán ph√¢n c·ª•m..."):
                    time.sleep(2)

                    kmeans = KMeans(n_clusters=num_clusters, random_state=42)
                    df1['Cluster'] = kmeans.fit_predict(df1[selected_features])

                    st.success("‚úÖ Ph√¢n c·ª•m ho√†n th√†nh!")
                    st.write("### D·ªØ Li·ªáu Sau Ph√¢n C·ª•m:")
                    st.write(df1)

                    # V·∫Ω bi·ªÉu ƒë·ªì
                    plt.figure(figsize=(10, 6))
                    plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=df1['Cluster'], cmap='viridis', s=50)
                    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, label='Centroids')
                    plt.xlabel(selected_features[0])
                    plt.ylabel(selected_features[1])
                    plt.title("K-Means Clustering")
                    plt.legend()
                    st.pyplot(plt)

                    # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì scatterplot v·ªõi 2 thu·ªôc t√≠nh ƒë·∫ßu ti√™n
                    if len(selected_features) >= 2:
                        plt.title("Scatterplot")
                        plt.legend()
                        plt.figure(figsize=(10, 6))
                        sns.scatterplot(
                            x=selected_features[0], 
                            y=selected_features[1], 
                            hue='Cluster', 
                            data=df1, 
                            palette='viridis', 
                            s=100
                        )
                        plt.xlabel(selected_features[0])
                        plt.ylabel(selected_features[1])
                        st.pyplot(plt)

                    # Hi·ªÉn th·ªã ch√∫ th√≠ch c·ªßa c√°c c·ªôt ƒë√£ chuy·ªÉn ƒë·ªïi
                    if column_notes:
                        st.write("\n### Ch√∫ th√≠ch cho c√°c c·ªôt chuy·ªÉn ƒë·ªïi:")
                        for col, notes in column_notes.items():
                            st.write(f"- **{col}**: {notes}")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()

# 3. Apriori Association Rule
def apriori_page():
    st.markdown("<h2>üîó Khai Ph√° Lu·∫≠t K·∫øt H·ª£p v·ªõi Apriori</h2>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write(df)
        # Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh d·∫°ng ph√π h·ª£p cho Apriori
        st.write("### Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu:")
        transactions = df['Items'].apply(lambda x: x.split(', '))
        unique_items = sorted(set(item for sublist in transactions for item in sublist))

        # T·∫°o DataFrame d·∫°ng one-hot encoding
        encoded_data = pd.DataFrame([[1 if item in transaction else 0 for item in unique_items] for transaction in transactions], columns=unique_items)
        st.write(encoded_data.head())
        
        # Ch·ªçn ng∆∞·ª°ng h·ªó tr·ª£ t·ªëi thi·ªÉu
        min_support = st.slider("Ch·ªçn min support:", 0.01, 0.5, 0.1, step=0.01)

        # Ch·ªçn ng∆∞·ª°ng ƒë·ªô tin c·∫≠y t·ªëi thi·ªÉu
        min_confidence = st.slider("Ch·ªçn min confidence:", 0.1, 1.0, 0.6, step=0.05)

        if st.button("üöÄ Th·ª±c Hi·ªán Apriori"):
            with st.spinner("üîÑ ƒêang khai ph√° t·∫≠p ph·ªï bi·∫øn v√† lu·∫≠t k·∫øt h·ª£p..."):
                time.sleep(2)
                st.success("‚úÖ Khai ph√° lu·∫≠t k·∫øt h·ª£p ho√†n th√†nh!")
                # Khai th√°c t·∫≠p ph·ªï bi·∫øn
                frequent_itemsets = apriori(encoded_data, min_support=min_support, use_colnames=True)
                st.write("### T·∫≠p Ph·ªï Bi·∫øn:")
                st.write(frequent_itemsets)

                # Khai th√°c lu·∫≠t k·∫øt h·ª£p
                rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=min_confidence)
                st.write("### Lu·∫≠t K·∫øt H·ª£p:")
                st.write(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

                # Hi·ªÉn th·ªã c√°c lu·∫≠t k·∫øt h·ª£p d∆∞·ªõi d·∫°ng b·∫£ng
                st.write("### Lu·∫≠t K·∫øt H·ª£p Chi Ti·∫øt:")
                for i, row in rules.iterrows():
                    st.write(f"**Lu·∫≠t {i+1}:** N·∫øu mua {set(row['antecedents'])} th√¨ s·∫Ω mua {set(row['consequents'])}")
                    st.write(f"- **Support:** {row['support']:.2f}")
                    st.write(f"- **Confidence:** {row['confidence']:.2f}")
                    st.write(f"- **Lift:** {row['lift']:.2f}")
                    st.write("---")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()
#4 thu·∫≠t to√°n Kohonen
def kohonen_page():
    st.header("üß† Kohonen Self-Organizing Map (SOM)")

    # T·∫£i d·ªØ li·ªáu t·ª´ file CSV
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write("### üîç D·ªØ li·ªáu G·ªëc:")
        st.write(df.head())

        # Ch·ªçn c√°c c·ªôt ƒë·ªÉ hu·∫•n luy·ªán SOM
        st.write("### üîß Ch·ªçn C√°c C·ªôt ƒë·ªÉ Hu·∫•n Luy·ªán SOM:")
        selected_columns = st.multiselect(
            "Ch·ªçn c√°c c·ªôt ƒë·ªÉ hu·∫•n luy·ªán:", 
            df.columns, 
            default=["Age", "AnnualIncome", "SpendingScore", "PurchaseFrequency", "AveragePurchaseValue", "Tenure", "OnlinePurchases"]
        )

        if len(selected_columns) < 2:
            st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn √≠t nh·∫•t 2 c·ªôt ƒë·ªÉ hu·∫•n luy·ªán SOM.")
            return

        # X·ª≠ l√Ω d·ªØ li·ªáu ph√¢n lo·∫°i b·∫±ng Label Encoding
        categorical_columns = df.select_dtypes(include=['object']).columns
        label_encoders = {}
        for col in categorical_columns:
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col])
            label_encoders[col] = le

        # Ch·ªçn d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán
        X = df[selected_columns]

        # Chu·∫©n h√≥a d·ªØ li·ªáu
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Hi·ªÉn th·ªã d·ªØ li·ªáu sau khi chu·∫©n h√≥a
        st.write("### üìä D·ªØ Li·ªáu Sau Khi Chu·∫©n H√≥a:")
        st.write(pd.DataFrame(X_scaled, columns=selected_columns).head())

        # C·∫•u h√¨nh tham s·ªë SOM
        st.header("‚öôÔ∏è C·∫•u H√¨nh SOM")
        map_size = st.slider("K√≠ch th∆∞·ªõc b·∫£n ƒë·ªì SOM:", min_value=3, max_value=15, value=7)
        iterations = st.slider("S·ªë l·∫ßn l·∫∑p:", min_value=100, max_value=10000, value=1000, step=100)

        # T·∫°o v√† hu·∫•n luy·ªán SOM
        som = MiniSom(map_size, map_size, X_scaled.shape[1], sigma=1.0, learning_rate=0.5)
        som.random_weights_init(X_scaled)

        if st.button("üöÄ Hu·∫•n Luy·ªán SOM"):
            with st.spinner("ƒêang hu·∫•n luy·ªán SOM..."):
                time.sleep(2)
                som.train_random(X_scaled, iterations)
            st.success("‚úÖ Hu·∫•n luy·ªán ho√†n t·∫•t!")

            # V·∫Ω b·∫£n ƒë·ªì SOM
            st.write("### üåê B·∫£n ƒê·ªì Kohonen (SOM)")

            plt.figure(figsize=(10, 10))
            for i, x in enumerate(X_scaled):
                w = som.winner(x)
                plt.text(w[0], w[1], str(df['CustomerID'].iloc[i]), 
                         color=plt.cm.rainbow(df['SpendingScore'].iloc[i] / 100),
                         fontdict={'weight': 'bold', 'size': 9})
            plt.xlim(0, map_size)
            plt.ylim(0, map_size)
            plt.title("Kohonen Self-Organizing Map")
            st.pyplot(plt)

            st.write("M√†u s·∫Øc th·ªÉ hi·ªán gi√° tr·ªã `SpendingScore` c·ªßa t·ª´ng kh√°ch h√†ng.")
    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    # N√∫t quay l·∫°i trang ch√≠nh
    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()
# 5. Naive Bayes Classification
def naive_bayes_page():
    st.title("ü§ñ Naive Bayes Classification")
    st.header("1Ô∏è‚É£ T·∫£i T·∫≠p D·ªØ Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn file CSV ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write("### üîç D·ªØ Li·ªáu ƒê√£ T·∫£i")
        st.dataframe(df)

        # Ch·ªçn c·ªôt quy·∫øt ƒë·ªãnh
        decision_column = st.selectbox("Ch·ªçn c·ªôt quy·∫øt ƒë·ªãnh:", df.columns)

        # T√°ch c·ªôt quy·∫øt ƒë·ªãnh v√† c·ªôt ƒëi·ªÅu ki·ªán
        condition_columns = [col for col in df.columns if col != decision_column]

        st.write("### üìä C·ªôt ƒêi·ªÅu Ki·ªán:")
        st.write(condition_columns)
        st.write("### üè∑Ô∏è C·ªôt Quy·∫øt ƒê·ªãnh:")
        st.write(decision_column)

        # ==========================
        # 2. Hu·∫•n luy·ªán m√¥ h√¨nh Naive Bayes
        # ==========================
        st.header("2Ô∏è‚É£ Hu·∫•n Luy·ªán M√¥ H√¨nh Naive Bayes")

        # Ch·ªçn 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ v·∫Ω ranh gi·ªõi quy·∫øt ƒë·ªãnh
        selected_features = st.multiselect("Ch·ªçn 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ v·∫Ω Decision Boundary:", condition_columns, default=condition_columns[:2])

        # Ch·ªçn ki·ªÉu Naive Bayes
        nb_type = st.selectbox(
            "Ch·ªçn lo·∫°i Naive Bayes:",
            ["GaussianNB", "MultinomialNB", "BernoulliNB"]
        )

        # Ch·ªçn t√πy ch·ªçn l√†m tr∆°n Laplace
        laplace_smoothing = st.selectbox(
            "L√†m tr∆°n Laplace:",
            ["C√≥", "Kh√¥ng"],
            index=0
        )

        # Ch·ªçn t·ª∑ l·ªá train-test split
        test_size = st.slider(
            "Ch·ªçn t·ª∑ l·ªá ki·ªÉm tra (test set):",
            0.1, 0.5, 0.3, step=0.1
        )

        if st.button("ü§ñ Th·ª±c hi·ªán Naive Bayes Classification"):
            if len(selected_features) != 2:
                st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn ƒë√∫ng 2 ƒë·∫∑c tr∆∞ng ƒë·ªÉ v·∫Ω ranh gi·ªõi quy·∫øt ƒë·ªãnh.")
            else:
                with st.spinner("üîÑ ƒêang th·ª±c hi·ªán ph√¢n l·ªõp Bayes..."):
                    time.sleep(2)

                    # T·ª± ƒë·ªông encode categorical data th√†nh s·ªë
                    encoders = {}
                    for column in df.columns:
                        if df[column].dtype == 'object':  # N·∫øu c·ªôt l√† d·∫°ng categorical
                            encoders[column] = LabelEncoder()
                            df[column] = encoders[column].fit_transform(df[column])

                    X = df[selected_features]
                    y = df[decision_column]

                    
                    # Chu·∫©n h√≥a d·ªØ li·ªáu
                    scaler = StandardScaler()
                    X = pd.DataFrame(scaler.fit_transform(X), columns=selected_features)

                    # Train-test split
                    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0)

                    # Thi·∫øt l·∫≠p gi√° tr·ªã var_smoothing
                    var_smoothing = 1e-9 if laplace_smoothing == "C√≥" else 1e-12
                    multi_alpha = 1.0 if laplace_smoothing == "C√≥" else 0.0
                    ber_alpha = 1.0 if laplace_smoothing == "C√≥" else 0.0

                    # T·∫°o m√¥ h√¨nh d·ª±a tr√™n l·ª±a ch·ªçn
                    if nb_type == "GaussianNB":
                        clf = GaussianNB(var_smoothing=var_smoothing)
                    elif nb_type == "MultinomialNB":
                        clf = MultinomialNB(alpha=multi_alpha)
                    elif nb_type == "BernoulliNB":
                        clf = BernoulliNB(alpha=ber_alpha)

                    # Hu·∫•n luy·ªán m√¥ h√¨nh
                    clf.fit(X_train, y_train)

                    # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
                    y_pred = clf.predict(X_test)
                    acc = clf.score(X_test, y_test)

                    st.success("‚úÖ Ph√¢n l·ªõp Bayes ho√†n th√†nh!")
                    # Hi·ªÉn th·ªã ƒë·ªô ch√≠nh x√°c
                    st.write(f"### üéØ ƒê·ªô Ch√≠nh X√°c: {acc:.2f}")

                    # Hi·ªÉn th·ªã b√°o c√°o chi ti·∫øt
                    st.write("### üìã B√°o C√°o Chi Ti·∫øt:")
                    st.text(classification_report(y_test, y_pred))

                    # Tr·ª±c quan h√≥a confusion matrix
                    st.write("### üîé Confusion Matrix:")
                    cm = confusion_matrix(y_test, y_pred)
                    fig, ax = plt.subplots()
                    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))
                    disp.plot(ax=ax, cmap='viridis')
                    st.pyplot(fig)

                    # Tr·ª±c quan h√≥a d·ªØ li·ªáu b·∫±ng scatter plot
                    st.write("### Bi·ªÉu ƒë·ªì Scatter Plot c·ªßa D·ªØ Li·ªáu Ki·ªÉm Tra:")
                    fig, ax = plt.subplots()
                    scatter = ax.scatter(X_test.iloc[:, 0], X_test.iloc[:, 1], c=y_pred, cmap='viridis', edgecolor='k', s=100)
                    ax.set_xlabel(selected_features[0])
                    ax.set_ylabel(selected_features[1])
                    legend1 = ax.legend(*scatter.legend_elements(), title="Classes")
                    ax.add_artist(legend1)
                    st.pyplot(fig)

                    # ==========================
                    # 3. V·∫Ω Decision Boundary
                    # ==========================
                    st.write("### üåê Decision Boundary")

                    # T·∫°o l∆∞·ªõi ƒëi·ªÉm ƒë·ªÉ v·∫Ω ranh gi·ªõi
                    x_min, x_max = X_train[selected_features[0]].min() - 1, X_train[selected_features[0]].max() + 1
                    y_min, y_max = X_train[selected_features[1]].min() - 1, X_train[selected_features[1]].max() + 1
                    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                                        np.arange(y_min, y_max, 0.01))

                    # D·ª± ƒëo√°n cho t·ª´ng ƒëi·ªÉm trong l∆∞·ªõi
                    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
                    Z = Z.reshape(xx.shape)

                    # V·∫Ω ranh gi·ªõi quy·∫øt ƒë·ªãnh
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')
                    scatter = ax.scatter(X_test[selected_features[0]], X_test[selected_features[1]], c=y_test, edgecolor='k', cmap='viridis', s=100)
                    ax.set_xlabel(selected_features[0])
                    ax.set_ylabel(selected_features[1])
                    legend1 = ax.legend(*scatter.legend_elements(), title="Classes")
                    ax.add_artist(legend1)
                    st.pyplot(fig)

    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

    # N√∫t quay l·∫°i trang ch√≠nh
    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()
# 6. Random Forest
def random_forest_page():
    st.header("üå≤ Random Forest")

    # Ch·ªçn b·ªô d·ªØ li·ªáu
    dataset_name = st.selectbox(
        "üîπ Ch·ªçn b·ªô d·ªØ li·ªáu:",
        ["Iris", "Wine", "Breast Cancer"]
    )

    # Load d·ªØ li·ªáu t∆∞∆°ng ·ª©ng
    if dataset_name == "Iris":
        data = load_iris()
    elif dataset_name == "Wine":
        data = load_wine()
    else:
        data = load_breast_cancer()

    # Hi·ªÉn th·ªã th√¥ng tin v·ªÅ d·ªØ li·ªáu
    X = data.data
    y = data.target
    feature_names = data.feature_names
    target_names = data.target_names

    st.write("### üîç D·ªØ li·ªáu:")
    df = pd.DataFrame(X, columns=feature_names)
    df['Target'] = y
    st.write(df.head())

    # Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test
    test_size = st.slider("üîπ T·ª∑ l·ªá d·ªØ li·ªáu test:", min_value=0.1, max_value=0.5, value=0.3, step=0.05)
    random_state = st.number_input("üîπ Random State:", min_value=0, max_value=1000, value=0, step=1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # Tham s·ªë cho Random Forest
    st.write("### ‚öôÔ∏è C·∫•u h√¨nh Random Forest")
    n_estimators = st.slider("üî∏ S·ªë l∆∞·ª£ng c√¢y (n_estimators):", min_value=10, max_value=500, value=100, step=10)
    max_depth = st.slider("üî∏ ƒê·ªô s√¢u t·ªëi ƒëa (max_depth):", min_value=1, max_value=20, value=5, step=1)
    min_samples_split = st.slider("üî∏ S·ªë m·∫´u t·ªëi thi·ªÉu ƒë·ªÉ chia (min_samples_split):", min_value=2, max_value=10, value=2, step=1)

    # Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
    if st.button("üöÄ Hu·∫•n Luy·ªán M√¥ H√¨nh"):
        with st.spinner("üîÑ ƒêang hu·∫•n luy·ªán..."):
            time.sleep(2)
            clf = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=max_depth,
                min_samples_split=min_samples_split,
                random_state=random_state
            )
            clf.fit(X_train, y_train)

            # ƒê·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh
            acc = clf.score(X_test, y_test)
            st.success(f"‚úÖ ƒê·ªô ch√≠nh x√°c: {acc:.2f}")

            # V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n
            st.write("### üìä Ma Tr·∫≠n Nh·∫ßm L·∫´n")
            y_pred = clf.predict(X_test)
            cm = confusion_matrix(y_test, y_pred)
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)

            fig, ax = plt.subplots(figsize=(8, 6))
            disp.plot(ax=ax, cmap='viridis')
            st.pyplot(fig)

            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì Feature Importance
            st.write("### üåü T·∫ßm Quan Tr·ªçng C·ªßa C√°c ƒê·∫∑c Tr∆∞ng (Feature Importance)")
            importances = clf.feature_importances_
            feature_importance_df = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importances
            }).sort_values(by='Importance', ascending=False)

            # V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.barplot(
                x='Importance',
                y='Feature',
                data=feature_importance_df,
                palette='viridis'
            )
            ax.set_title("Feature Importance")
            st.pyplot(fig)

    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()
# trang t·∫≠p th√¥
def rough_set_page():
    st.title("üìä Rough Set Analysis Application")
    st.header("1Ô∏è‚É£ T·∫£i T·∫≠p D·ªØ Li·ªáu")
    uploaded_file = st.file_uploader("Ch·ªçn file excel ƒë·ªÉ t·∫£i d·ªØ li·ªáu", type=["csv"])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.write("### üîç D·ªØ Li·ªáu ƒê√£ T·∫£i")
        st.dataframe(df)
        # Ch·ªçn c·ªôt quy·∫øt ƒë·ªãnh
        decision_column = st.selectbox("Ch·ªçn c·ªôt quy·∫øt ƒë·ªãnh:", df.columns)

        # T√°ch c·ªôt quy·∫øt ƒë·ªãnh v√† c·ªôt ƒëi·ªÅu ki·ªán
        condition_columns = [col for col in df.columns if col != decision_column]

        st.write("### üìä C·ªôt ƒêi·ªÅu Ki·ªán:")
        st.write(condition_columns)
        st.write("### üè∑Ô∏è C·ªôt Quy·∫øt ƒê·ªãnh:")
        st.write(decision_column)

        # ==========================
        # 2. T√≠nh quan h·ªá b·∫•t kh·∫£ ph√¢n bi·ªát
        # ==========================
        st.header("2Ô∏è‚É£ Quan H·ªá B·∫•t Kh·∫£ Ph√¢n Bi·ªát")

        def indiscernibility_relation(data, columns):
            indiscernibility_classes = {}
            for i, row in data.iterrows():
                key = tuple(row[columns])
                if key not in indiscernibility_classes:
                    indiscernibility_classes[key] = []
                indiscernibility_classes[key].append(i)
            return indiscernibility_classes

        if st.button("T√≠nh Quan H·ªá B·∫•t Kh·∫£ Ph√¢n Bi·ªát"):
            ind_classes = indiscernibility_relation(df, condition_columns)
            st.write("### üîó C√°c L·ªõp Quan H·ªá B·∫•t Kh·∫£ Ph√¢n Bi·ªát:")
            for key, indices in ind_classes.items():
                st.write(f"{key}: {indices}")

        # ==========================
        # 3. X·∫•p x·ªâ t·∫≠p h·ª£p
        # ==========================
        st.header("3Ô∏è‚É£ X·∫•p X·ªâ T·∫≠p H·ª£p")

        target_value = st.text_input("Nh·∫≠p gi√° tr·ªã c·ªßa c·ªôt quy·∫øt ƒë·ªãnh ƒë·ªÉ t√≠nh x·∫•p x·ªâ:", "")

        def lower_upper_approximation(data, condition_columns, decision_column, target_value):
            target_indices = set(data[data[decision_column] == target_value].index)
            ind_classes = indiscernibility_relation(data, condition_columns)

            lower_approx = set()
            upper_approx = set()

            for indices in ind_classes.values():
                indices_set = set(indices)
                if indices_set.issubset(target_indices):
                    lower_approx.update(indices_set)
                if indices_set.intersection(target_indices):
                    upper_approx.update(indices_set)

            return lower_approx, upper_approx

        if st.button("T√≠nh X·∫•p X·ªâ"):
            lower_approx, upper_approx = lower_upper_approximation(df, condition_columns, decision_column, target_value)
            boundary_region = upper_approx - lower_approx

            st.write(f"### ‚¨áÔ∏è X·∫•p X·ªâ D∆∞·ªõi: {lower_approx}")
            st.write(f"### ‚¨ÜÔ∏è X·∫•p X·ªâ Tr√™n: {upper_approx}")
            st.write(f"### üîπ V√πng Bi√™n: {boundary_region}")

        # ==========================
        # 4. T√¨m reducts
        # ==========================
        st.header("4Ô∏è‚É£ T√¨m Reducts")

        def find_reducts(data, condition_columns, decision_column):
            full_ind = indiscernibility_relation(data, condition_columns)
            reducts = []

            for i in range(1, len(condition_columns) + 1):
                for subset in combinations(condition_columns, i):
                    subset_ind = indiscernibility_relation(data, list(subset))
                    if subset_ind == full_ind:
                        reducts.append(subset)

            return reducts

        if st.button("T√¨m Reducts"):
            reducts = find_reducts(df, condition_columns, decision_column)
            st.write("### üîç C√°c Reduct T√¨m ƒê∆∞·ª£c:")
            for reduct in reducts:
                st.write(reduct)

        # ==========================
        # 5. Sinh lu·∫≠t quy·∫øt ƒë·ªãnh
        # ==========================
        st.header("5Ô∏è‚É£ Sinh Lu·∫≠t Quy·∫øt ƒê·ªãnh")

        def generate_decision_rules(data, condition_columns, decision_column):
            rules = []
            for i, row in data.iterrows():
                condition = " AND ".join([f"{col}={row[col]}" for col in condition_columns])
                decision = f"{decision_column}={row[decision_column]}"
                rule = f"IF {condition} THEN {decision}"
                rules.append(rule)
            return rules

        if st.button("Sinh Lu·∫≠t Quy·∫øt ƒê·ªãnh"):
            rules = generate_decision_rules(df, condition_columns, decision_column)
            st.write("### üìú Lu·∫≠t Quy·∫øt ƒê·ªãnh:")
            for rule in rules:
                st.write(rule)

    else:
        st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i file CSV ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    if st.button("‚¨ÖÔ∏è Back to Home"):
        st.session_state["current_page"] = main_page
        st.rerun()
# ƒêi·ªÅu h∆∞·ªõng gi·ªØa c√°c trang
if "current_page" not in st.session_state:
    st.session_state["current_page"] = main_page

st.session_state["current_page"]()
